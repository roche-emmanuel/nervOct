*** TODOs:
- Implement dropout
- Apply weight multiplier dropout for test time.
- Implement input feature lost
- implement octave plugin to access gradient descent.
- Provide traits for methods with many arguments.
- Test copy construction of BPTraits ?
- Add d_traits usage for errfunc_device.
- Test the value from computeTrainCost and computeCvCost
- check current perfs on errfunc call.
- Move X/y_cv into BPTraits.


*** CUDA infos:

Showing CUDA info...
device...................: 0
Name.....................: GeForce GTX 660 [1110Mhz - supports CUDA 3.0
Multi-Processors.........: 5
Global mem...............: 2147483648
Const mem................: 65536
Shared mem per block.....: 49152
Regs per block...........: 65536
Max threads per block....: 1024
Max threads dim..........: (1024, 1024, 64)
Max grid size............: (2147483647, 65535, 65535)
Warp size................: 32
Mem pitch................: 2147483647
Texture Alignment........: 512
Device overlap...........: 1
kernel Timeout Enabled...: 0
Device integrated........: 0
Can map host memory......: 1
Compute mode.............: 0
Size of floating type....: 8


*** Investigate the perfs of nnCostFunction

-> initial implementation:
- Tested with architecture: [1441; 200; 3];
-	m = 2000;
- Results (x64):

Elapsed time is 6.86839 seconds.
   #        Function Attr     Time (s)        Calls
---------------------------------------------------
   7        binary *             5.696          130
  15             exp             0.467           30
  11       postfix '             0.320           40
   2  nnCostFunction             0.162           10
  16       binary ./             0.058           30
  21       binary .*             0.030           60
   6        binary +             0.026          220
  14        prefix -             0.021           40
  23        binary /             0.020           40
  13         sigmoid             0.012           30
  26           zeros             0.011           40
   4        binary -             0.009           90
  25 sigmoidGradient             0.007           10
  20             log             0.004           20
  22             sum             0.003           60
   1             tic             0.000            1
   3           numel             0.000           10
   5            cell             0.000           40
   8         reshape             0.000           20
   9            size             0.000           80

- Results (x86):

Elapsed time is 8.49249 seconds.
   #        Function Attr     Time (s)        Calls
---------------------------------------------------
   7        binary *             7.328          130
  15             exp             0.460           30
  11       postfix '             0.316           40
   2  nnCostFunction             0.162           10
  16       binary ./             0.057           30
   6        binary +             0.030          220
  21       binary .*             0.028           60
  14        prefix -             0.020           40
  23        binary /             0.020           40
  26           zeros             0.016           40
  13         sigmoid             0.013           30
   4        binary -             0.009           90
  25 sigmoidGradient             0.007           10
  20             log             0.003           20
  27             toc             0.001            1
  22             sum             0.001           60
   1             tic             0.000            1
   3           numel             0.000           10
   5            cell             0.000           40
   8         reshape             0.000           20

-> C++ implementation:
- Tested with architecture: [1441; 200; 3];
-	m = 2000;
- Results (x64):

Elapsed time is 6.66538 seconds.
   #            Function Attr     Time (s)        Calls
-------------------------------------------------------
   2    nn_cost_function             6.664           10
   3                 toc             0.001            1
   1                 tic             0.000            1
   4             profile             0.000            1
   5              nargin             0.000            1
   6           binary !=             0.000            1
   7               false             0.000            1
   8 __profiler_enable__             0.000            1

-> nn_cg_train implementation:

ans =

   2000   1441

ans =

      3   2000

Elapsed time is 2.26013 seconds.
   #            Function Attr     Time (s)        Calls
-------------------------------------------------------
   2         nn_cg_train             2.260            1
   1                 tic             0.000            1
   3                 toc             0.000            1
   4             profile             0.000            1
   5              nargin             0.000            1
   6           binary !=             0.000            1
   7               false             0.000            1
   8 __profiler_enable__             0.000            1


==> Cost function on GPU with events : (kept for reference)

void costFunc_device(unsigned int nl, unsigned int np, unsigned int* lsizes, unsigned int nsamples,
  double* d_params, double* d_X, double* d_yy, double lambda, double& J, double* d_grads, double* d_deltas, double* d_inputs, double* d_regw)
{
  // getLastCudaError("Checkpoint1");

  unsigned int nt = nl-1; // number of matrices evolved.

  // offset used to locate the theta_i matrix in the d_params array.
  unsigned int theta_offset = 0;

  // Offset used for the z(i) matrix on iteration i
  int input_offset = 0;

  int next_input_offset = 0; //nsamples*lsizes[1];

  // Prepare the streams:
  cudaStream_t s1;
  cudaStream_t s2;
  cudaStreamCreate(&s1);
  cudaStreamCreate(&s2);

  for(unsigned int i=0; i<nt;++i) {
    // We compute the activation and input values for the given layer:

    // The kernel compute the values of zi and a(i+1) 
    // (note that the value or a(0) is already loaded in the Activation vector).
    // even if we compute the a(i+1) matrix we actually discard completely the first column
    // in this matrix (colu of intercept terms). As a result we just need to mapped the GPU grid to
    // the dimension of of the sub z(i) matrix (which is transposed.)
    // THe dimensions for z(i) are: lsize(i+1) * nsamples
    // When this is transposed we get: nsamples * lsize(i+1);
    unsigned int nrows = lsizes[i+1];
    unsigned int ncolT = lsizes[i]; // we remove 1 here because we consider the intercept row as "virtual" in our calculation.
    unsigned int ncols = nsamples;

    dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);
    dim3 dimGrid((BLOCK_SIZE + ncols-1)/BLOCK_SIZE, (BLOCK_SIZE + nrows-1)/BLOCK_SIZE);

    // Also we will need access to the theta_i matrix so we need to keep track of its global offset in the
    // network parameters array.
    // logDEBUG("Using grid size: ("<<dimGrid.x<<" x "<<dimGrid.y<<")");
    ComputeActivation<<<dimGrid, dimBlock,0,s1>>>(theta_offset, input_offset, next_input_offset,
      nrows, ncols, ncolT, d_params, d_inputs, d_X);
    // CHECK_KERNEL();

    // update the offsets:
    theta_offset += lsizes[i+1]*(lsizes[i]+1);
    input_offset = next_input_offset;
    next_input_offset += nrows*ncols;
  }

  double* d_hx = d_inputs + input_offset;

  // Here we can compute the cost now:
  // The hx matrix is mapped to the last z matrix. eg at i=nt-1
  // So its dimensions are lsizes[nt-1+1] * nsamples = lsizes[nl-1] * nsamples
  // same dimensions for the yy matrix, and we want to perform reduction other those 2 matrices
  J = 0.0;
  unsigned int count = nsamples*lsizes[nt];
  reduction_cost_device(d_hx, d_yy, count, J);
  // CHECK_KERNEL()

  J /= (double)nsamples;

  double Jreg = 0.0;
  reduction_cost_reg_device(d_params, d_regw, np, Jreg);
  // CHECK_KERNEL()

  J += (Jreg*lambda)/(2.0*nsamples);

  // Prepare the computation of the delta matrices in reverse order:

  // Offset to use when reading the delta matrix in the current iteration
  // except when next_delta_offset is 0, in that case we read the hx and yy matrices.
  unsigned int delta_offset = 0;

  // Offset to use when writing the delta matrix in the current iteration
  unsigned int next_delta_offset = 0;

  // remove the last theta matrix size from the theta offset so that we can use
  // that offset to retrieve the proper theta matrix:
  theta_offset -= lsizes[nt]*(lsizes[nt-1]+1);

  // initially the input_offset is pointing on the hx matrix which is z(nt-1) with our convention (eg. z(0) is not in the array.)
  // But the first one we will need is actually the one before that: z(nt-2)
  // So we need to update the offset, and remove the size of the matrix z(nt-2) ! (pointer is at the beginning of z(nt-1))
  // Note: This is now done inside the loop:
  // input_offset -= lsizes[nt-1]*nsamples;

  // Prepare the offset for the gradient array:
  // keep in mind we start with the latest theta matrix:
  unsigned int grad_offset = np - lsizes[nt]*(lsizes[nt-1]+1);
  cudaEvent_t* events = new cudaEvent_t[nt];

  for(unsigned int i=nt;i>0;--i) {
    unsigned int nrows = lsizes[i];
    unsigned int ncols = nsamples;
    unsigned int niter = lsizes[i+1];
    unsigned int count = nrows*ncols;

    dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);
    dim3 dimGrid((BLOCK_SIZE + ncols-1)/BLOCK_SIZE, (BLOCK_SIZE + nrows-1)/BLOCK_SIZE);

    // Prepare the event to use here:
    cudaEventCreate(&events[i-1]);

    if(i==nt) {
      // we should just copy the difference of hx and yy into the z matrix.
      // CHECK_KERNEL()
      InitLastDelta<<<dimGrid, dimBlock,0,s1>>>(nrows, ncols, d_deltas, d_hx, d_yy);
      // CHECK_KERNEL()
    }
    else {
      // We compute the delta from the previous delta:
      // We start this computation for delta(nt-1). this matrix is build from theta(nt-1) and delta(nt).
      // also in the process we use the input matrix z(nt-2)
      ComputeDelta<<<dimGrid, dimBlock,0,s1>>>(theta_offset, input_offset, delta_offset, next_delta_offset, nrows, ncols, niter, d_params, d_inputs, d_deltas);
      // CHECK_KERNEL()

      // once the computation is done for that layer we move to the previous layer:
      theta_offset -= lsizes[i]*(lsizes[i-1]+1);
    }

    //register the event:
    cudaEventRecord(events[i-1],s1);

    delta_offset = next_delta_offset;
    next_delta_offset += count;

    // At this point we have the previous theta matrix (eg. theta(i-1) pointed by theta_offset. (both when i=nt and i<nt).
    // and thats the matrix we need to compute the gradient values.
    // the gradient mat has the same size as the current theta matrix.
    // similarly, the input_offset is pointing on z(i-2) which is the one we need to perform the computation too.
    // and delta_offset points to the delta matrix we just wrote (eg. delta(i)).
    nrows = lsizes[i];
    ncols = lsizes[i-1]+1;
    niter = nsamples;
    count = nrows*ncols;

    // Compute the gradient:
    dimBlock = dim3(BLOCK_SIZE, BLOCK_SIZE);
    dimGrid = dim3((BLOCK_SIZE + ncols-1)/BLOCK_SIZE, (BLOCK_SIZE + nrows-1)/BLOCK_SIZE);

    input_offset -= lsizes[i-1]*nsamples; // we remove the size of the next delta matrix to be computed. which is also the size of the next z matrix we will use.
    // logDEBUG("GPU: Gradient at i="<<i<<" of size "<< nrows <<" x " << ncols<<", offset="<<grad_offset<<", input_offset="<<input_offset<<", nsamples="<<nsamples);

    // wait for the event on s1:
    cudaStreamWaitEvent(s2,events[i-1],0);

    ComputeGradient<<<dimGrid, dimBlock,0,s2>>>(theta_offset, input_offset, delta_offset, grad_offset, nrows, ncols, niter, d_X, d_params, d_inputs, d_deltas, d_grads, lambda);
    // CHECK_KERNEL()

    // update the gradient offset by removing the size of the next gradient matrix to be computed:
    // except for the last iteration where the value is not available:
    if(i>1) {
      grad_offset -= lsizes[i-1]*(lsizes[i-2]+1);
    }
  }

  // destroy all events:
  for(unsigned int i=0; i<nt;++i) {
     cudaEventDestroy(events[i]);
  }

  delete [] events;
  cudaStreamDestroy(s1);
  cudaStreamDestroy(s2);
}
