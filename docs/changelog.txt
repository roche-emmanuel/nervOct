== nervtech release v1.0.0 - ??? ==

- Initial commit of sgtCore.
- Built sgtCore library.
- Added nervMBP project.
- Added test for sgtcore library.
- Adding test for nervMBP.
- Added CUDA files for nervMBP library.
- Adding source file for oct_train_bp.
- Added content from nervtrade project.
- Adding test support for train_bp.
- Large refactoring now using Octave 3.8.2-mingw version to get the nervMBP loading properly with GetProcAddress
- Getting ready to continue...
- Fixed issue with RMS validation computation.
- Updated script files to handle x64 versions.
- Updated ignore file.
- Added implementation for show_cuda_info plugin.
- Added initial profile info for current nnCostFunction implementation.
- Implementing nn_cost_function cpp method.
- Completed initial cpp implementation of nn_cost_function.
- Micro optimizations on nn_coost_function.
- Added initial nervCUDA module.
- Added CUDA matrix multiplication implementation.
- Added support for matrix transpose operation.
- Updating cost_function_cuda implementation.
- Using tpB multMat version.
- Added implementation for tpA support in mult mat.
- Trying to avoid shared memory bank conflicts.
- Working on compute_activation kernel.
- Validated computation of activation and inputs in kernel.
- Moved cudaDeviceReset call in nervcuda test.
- Working implementation of compute_activation with no activation array.
- Cleaning code
- Using input values computed on GPU in nn_cost_function_cuda.
- Cleaned code in nn_cost_function_cuda.
- Added support for cost J computation on GPU.
- Preparing computation of deltas and gradients on GPU.
- Preparing test on deltas computation.
- Tested computation of delta matrices.
- Working on gradient computation tests.
- tested computation of gradients.
- Investigationg invalid address issue.
- Starting from working point for address issue.
- partially working gradient computation when not using the Input matrix.
- Preparing the test the CPU gradients.
- testing CPU gradient computation. Warning nnCostFunction modified here to handle ONLY reg matrix.
- Finally fixed the input array access issue.
- Tested grad computation in CPU with regularization.
- Working gradient computation on GPU.
- Tested GPU computation of gradient.
- Started code cleaning process.
- Working gradien computation WRT nnCostFunction.
- Preparing to clean nnCostFunction.
- Fixed costfunc_cpu test.
- Made inputs array optional for costfunc call.
- Refactored test nervcuda app.
- Added costfunc_device implementation.
- Started implementation of cgtrainCPU
- Working on test for nn_cg_train_cpu.
- Prepared comparaison test for cg_train_cpu.
- testing cg_train_cpu: success with 1 iteration.
- validated test for nn_cg_train_cpu.
- Preparing implementation for conjugateGradient on GPU.
- Separated conjugate gradient implementation in multi classes.
- Started implementation/test of cg_train GPU.
- Integrated regularization correction on GPU algorithm.
- Implemented methods for pure GPU conjugate gradient descent support.
- Added test for gpu basic vector operations.
- working on conjugateDradientGPU implementation.
- Updated cgGPU::resetS() method.
- Updated cgGPU::evaluateCost() method.
- Updated cgGPU::save and restoreParameters.
- removed _df0,1,2 on cpu.
- Removing need for _s array on CPU.
- Finalized cgGPU implementation with test.
- Validated Test of cgGPU in octave.
- Added some performances improvements and tests.
- Minor improvement on compute gradient kernel.
- Trying to improve compute delta kernel.
- Coalesced access to inputs and deltas in compute delta kernel.
- Coalesced access to grads in compute gradient kernel.
- Adding tests for flot matrix multiplication.
- minor optimization in computeActivation for global memory store.
- Keeping reference on GPU event usage.
- Removed vbssim message.
- Performing initial tests for gradient descent support.
- Added test for construction of the gd object.
- Building gradient descent class.
- Started implementation of gd_errfunc_device function.
- Refactoring kernels to templates.
- Moved the compute_dot kernel.
- Moved copy_vector method.
- Moved mix_vector kernel.
- Refactoring matrix multiplication kernels.
- Refactoring matrix multiplication tpB kernel.
- Refactoring matrix multiplication tpA kernel.
- Rename multiplyMatrices to matmult
- Renamed multipleMatricesf to matmult_f
- Renamed some files to cpp instead of cu.
- Removed helpers.cpp file.
- Renamed reduction to reduce_sum.
- Renamed reduction.cu to reduce_sum.cu
- Renamed reduce_sum_device to reduce_sum_launcher.
- renamed reduction_cost to reduce_cost.
- Refactored reduce_cost_reg kernel.
- Renamed ConjugateGradientGPU.cu to ConjugateGradientGPU.cpp.
- Added float implementation for most kernels.
- Performing initial tests on TrainingSet class.
- Using trainingset in tests.
- Added test for gd_errfunc.
- Starting computation of NAG in gradient descent.
- Adding mini batch support in gradient descent class.
- Added support for mini batch in gradient descent.
- Added support for streamin gradient descent.
- Made computation of J optional in gd_errfunc.
- Added support for cross validation sets generation in training set.
- Added implementation of early stopping for gradient descent.
- Updated early stopping support in gradient descent.
- Added test for early stopping with mini-batch.
- Avoiding gradient computation in cv cost evaluation.
- Added specification of the bias in gradient descent.
- Added implementation and test for gradientDescentf.
- Preparing for update of gradient descent classes.
- Working implementation for templated gradient descent version.
- Built real classes GDd and GDf for GradientDescent double and float versions.
- Added implementation for sparse initialization.
- Implemented nn_predict method and using it in gd_errfunc. Now need testing.
- Added dedicated prediction computation method and tests.
- Added random bias in nn_predict tests.
- Added support for weight multiplier in nn activation computation.
- Preparing trait struct for BP compute kernels.
- Replacing ComputeActivation with traits argument version.
- Refactored compute kernels to use traits.
- Cleaning after using traits for compute kernels.
- Preparing BPTraits.
- Done preparing BPTraits.
- refactoring gd_errfunc to use traits.
- Refactoring errfunc to use traits.
- Started to used createGPUBuffer.
- Using create/destroy GPU buffer methods.
- using BPDeviceTraits in _gd_errfunc
- Preparing to update GDTraits.
- Converted GDTraits to BPTraits base.
- Updating gradinet descent to use BPTraits.
- Starting usage of BPDeviceTraits in gradient descent class.
- Now using BPDeviceTraits for y_train and X/y_cv.
- Updated usage of BPDeviceTraits in Gradient Descent.
- Using BPDeviceTraits for errfunc.
- Added compute_cost and compute_grads options in BPTraits.
- Cleaning code.
- Moved X_cv and Y_cv into BPTraits.
- Removed BPTraits vars from Gradient descent class.
- Using BPDeviceTraits for nn_activation_device.
- Using BPTraits for nn_predict.
- Implemented BPComputeTraits operator=(BPTraits) method.
- Improved test duration.
- Added tests for computeTrainCost and computeCvCost.
- Added test for gd_errfunc performances.
- Added test for BPTraits copy construction.
- Added test for BPDeviceTraits.
- Preparing usage of weight multipliers and dropout in gradient descent.
- Added generation of rand state in BPDeviceTraits.
- Started implementation of dropout: dropping computed activation and bias partially (need to update compute gradient)
- Updated wbias to contain bias value directly.
- Refactoring some kernels.
- Refactoring compute_gradient kernel.
- Merge dropout computation on compute_activation kernel.
- Updating computation of wbias offset for compute_gradient kernel.
- Added test for rand_weights_device.
- Added test for rand_weights_device_debug
- Added test for nn_predict with dropout.
- Preparing reconstruction on gd_errfunc_cpu method.
- Completed refactoring of gd_errfunc_cpu method.
- Cleaning code.
- Preparing test for errfunc with dropout.
- Preparing test with dropout support.
- Added implementation and test for dropout.
- Added test for gradient descent with dropout.
- Added RandDeviceTraits.
- Refactoring rand_weights methods to use RandTraits.
- Using randTraits to select debug mode functions.
- Refactoring rand traits usage.
- Preparing to add X dropout.
- Using wX in compute traits instead of X.
- Added some debug macros.
- Added proper logging mechanism.
- Added support for wX usage in DeviceTraits.
- Getting ready for X random dropout.
- Added implementaion for input features dropout.
- Added test for threshold 0.0 and 1.0 on values rand_weights.
- Added test for ratio on rand weights with values buffer.
- Added test for debug rad weights with input values buffer.
- Added initial octave plugin for gradient descent support.
- Added retrieval of lsizes value.
- Added run_gradient_descent function and initial test.
- testing traits before device traits init in gradient descent.
- Building octave plugin for nn_gradient_descent.
- Retrieving most parameters in nn_gradient_descent.
- using value of _maxiter <= 0 as trigger in gradient descent.
- Retrieving all trait parameters from octave plugin for nn_gradient_descent.
- Returning newly computed params from nn_gradient_descent.
- Added trait for eval frequency and callback to retrieve cv cost at runtime.
- Added nnTrainNetworkNERV method.
- Implemented rescale weights functions to use after dropout.
- Working on mismatch of cv cost computation.
- Added comparaison of gd_errfunc and old costfunc test.
- Added test for final cv cost computation consistency.
- Added check on final cv cost computation from nn_gradient_descent.
- Still working on the Jcv mismatch issue.
- Finally fixed computation of Jcv : was due to transposition of yy matrix (eg. all computations were wrong so far).
- Started implementation of strategy manager.
- Started implementation of strategy evaluation.
- Added test for Strategy::getPrice
- Added implementation for strategy evaluation.
- Adding plugin trade_strategy for octave.
- Building trade_strategy function.
- Added support for destroy_strategy usage.
- Added support for strategy evaluation from octave.
- Adding strategy model class.
- Added model list in strategy.
- Added implementation for add_model.
- Added add_model support for octave plugin.
- Added support for mu and sigma in network model.
- Added test with real data for strategy evaluation.
- Studying network accuracy.
- Added simple test for XOR network computation.
- Added test with XOR and AND network.
- Added test with XOR and AND and OR network.
- Testing training settings.
- Preparing training 4 for minibatch usage.
- Found the source of the mini batch issue: X matrix should be transposed.
- Now using transposed X matrix for gradient descent. This should fix the mini batch issue.
- Performed test for gradient descent in octave.
- Tested training with 1024 features.
- Preparing for addition of learning rate decay.
- Added support for gradient descent advanced parameters.
- Tested learning with PCA activated.
- Preparing to add support for sotfmax.
- Added kernel for matrix vector multiplication with Cublas.
- Added test for matrix vector multiplication.
- Added declaration for matrix vector mult method.
- Added initialization for cublas handle.
- Preparing for usage of softmax computation in nn_activation.
- Updated nnPrepareTraining to use complete week datasets directly.
- Added improved feature and label generation functions.
- Added support for prices buffer for strategy evaluation and lot_multiplier.
- Fixing trade startegy evaluation process.
- testing trade strategy.
- Added training 10 on 24 weeks.
- Added train_network_1 method.
- Create eval strategy method.
- Added support for ping frequency in gradient descent.
- Added support to convert the price features to rate of returns.
- Added support for removal of number of minutes as feature.
- Restored the confidence multiplier.
- Testing various layer sizes.
- Added rangeToString and lsizesToString support functions.
- Added test for mat_elem_col_mult and _div methods.
- Added full implementation for softmax usage.